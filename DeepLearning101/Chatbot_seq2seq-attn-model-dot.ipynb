{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50269cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f52735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터\n",
    "hidden_size = 256\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "MAX_LENGTH = 300\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedf743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):  # NaN값을 처리\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', ' ', text)   #숫자를 공백으로\n",
    "    text = re.sub(r'([^\\w\\s])', r' \\1 ', text)   # 마침표 앞 뒤로 공백 추가\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 두개 이상의 공백은 하나의 공백으로..\n",
    "    text = text.strip()  # 텍스트 앞 뒤의 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6f3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df632606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605b7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320af9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        attn_weights = F.softmax(torch.bmm(encoder_outputs.unsqueeze(0), hidden[0][0].unsqueeze(2)).squeeze(2), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        new_hidden = (torch.vstack([attn_applied, attn_applied]), hidden[1])\n",
    "        output, hidden = self.lstm(embedded[0].unsqueeze(0), new_hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(2, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac0bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0,0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    \n",
    "    loss.backward()    # backpropagation only 1 line!\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa63af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 반복해주는 코드\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000):\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    #학습 스케줄러\n",
    "    scheduler_encoder = StepLR(encoder_optimizer, step_size=10, gamma=0.1)\n",
    "    scheduler_decoder = StepLR(decoder_optimizer, step_size=10, gamma=0.1)\n",
    "    min_loss = 1000000\n",
    "    for iter in range(1, n_iters+1):\n",
    "        training_pair = random.choice(pairs)\n",
    "        input_tensor = tensorFromSentence(word_to_ix, training_pair[0]).to(device)\n",
    "        target_tensor = tensorFromSentence(word_to_ix, training_pair[1]).to(device)\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(f'Iteration: {iter}, Loss: {print_loss_avg: .4f}, enc_lr: {scheduler_encoder.get_lr()}')\n",
    "            print_loss_total = 0\n",
    "            \n",
    "            if min_loss > print_loss_avg:\n",
    "                torch.save(encoder.state_dict(), './models/chkpt/encoder_seq2seq_attention_dot_'+str(iter)+'.pth')\n",
    "                torch.save(decoder.state_dict(), './models/chkpt/decoder_seq2seq_attention_dot_'+str(iter)+'.pth')\n",
    "                min_loss = print_loss_avg\n",
    "                        \n",
    "            scheduler_encoder.step()\n",
    "            scheduler_decoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f007a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(word_to_ix, sentence).to(device)\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0,0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []  # output sentence\n",
    "        \n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                 decoded_words.append(ix_to_word[topi.item()])   #여기는 최종 아웃풋의 인덱스가 들어갑니다\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        # 의미없는 단어들 생략\n",
    "        meaningful_words = [word for word in decoded_words if word not in ('<EOS>', '<UNK>')]\n",
    "        final_output = ' '.join(meaningful_words)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2666a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅함수\n",
    "def chat(encoder, decoder):\n",
    "    print(\"Let's chat! (type 'bye' to exit)\")\n",
    "    while True:\n",
    "        input_sentence = input(\"> \")\n",
    "        if input_sentence == 'bye':\n",
    "            break\n",
    "        output_sentence = evaluate(encoder, decoder, input_sentence)\n",
    "        print('<', output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba101c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 기본 전처리 부분을..\n",
    "df = pd.read_csv('./dataset/chatbot_dataset.txt', sep='\\t', names=['Question', 'Answer'])\n",
    "df['Encoder Inputs'] = df['Question'].apply(clean_text)\n",
    "df['Decoder Inputs'] = df['Answer'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a6ca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      i ' m fine . how about yourself ?\n",
       "1                i ' m pretty good . thanks for asking .\n",
       "2                    no problem . so how have you been ?\n",
       "3                   i ' ve been great . what about you ?\n",
       "4         i ' ve been good . i ' m in school right now .\n",
       "                             ...                        \n",
       "295        i first learned how to do it in high school .\n",
       "296    did you take some sort of art class or somethi...\n",
       "297                         that was my favorite class .\n",
       "298                        you have got to be talented .\n",
       "299                                             thanks .\n",
       "Name: Decoder Inputs, Length: 300, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Decoder Inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcbaae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = [sentence for sentence in df['Encoder Inputs']]\n",
    "output_sentence = [sentence + \"<EOS>\" for sentence in df['Decoder Inputs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46849f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi , how are you doing ?',\n",
       " \"i ' m fine . how about yourself ?\",\n",
       " \"i ' m pretty good . thanks for asking .\",\n",
       " 'no problem . so how have you been ?',\n",
       " \"i ' ve been great . what about you ?\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3151016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m fine . how about yourself ?<EOS>\",\n",
       " \"i ' m pretty good . thanks for asking .<EOS>\",\n",
       " 'no problem . so how have you been ?<EOS>',\n",
       " \"i ' ve been great . what about you ?<EOS>\",\n",
       " \"i ' ve been good . i ' m in school right now .<EOS>\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentence[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f78445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 생성\n",
    "all_words = set(' '.join(df['Encoder Inputs'].tolist()+df['Decoder Inputs'].tolist()).split())\n",
    "vocab = {'<PAD>': PAD_token, '<SOS>': SOS_token, '<EOS>': EOS_token, '<UNK>': UNK_token}\n",
    "vocab.update({word: i+4 for i, word in enumerate(all_words)})\n",
    "vocab_size = len(vocab)\n",
    "# vocab 변수 저장\n",
    "with open('./dataset/vocab_seq2seq_attention_dot.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b24d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = vocab\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1596f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110f56e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word[91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c261724",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(vocab_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderLSTM(hidden_size, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19af137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f8baca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs 리스트를 만들어서 학습 데이터를 준비\n",
    "pairs = [list(x) for x in zip(df['Encoder Inputs'], df['Decoder Inputs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee1de5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m fine . how about yourself ?\",\n",
       " \"i ' m pretty good . thanks for asking .\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6193a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gideon\\miniconda3\\envs\\fusion_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Loss:  3.1779, enc_lr: [0.001]\n",
      "Iteration: 2000, Loss:  2.5736, enc_lr: [0.001]\n",
      "Iteration: 3000, Loss:  1.8621, enc_lr: [0.001]\n",
      "Iteration: 4000, Loss:  1.2971, enc_lr: [0.001]\n",
      "Iteration: 5000, Loss:  0.8415, enc_lr: [0.001]\n",
      "Iteration: 6000, Loss:  0.5422, enc_lr: [0.001]\n",
      "Iteration: 7000, Loss:  0.3401, enc_lr: [0.001]\n",
      "Iteration: 8000, Loss:  0.1924, enc_lr: [0.001]\n",
      "Iteration: 9000, Loss:  0.1517, enc_lr: [0.001]\n",
      "Iteration: 10000, Loss:  0.1720, enc_lr: [0.001]\n",
      "Iteration: 11000, Loss:  0.1311, enc_lr: [1e-05]\n",
      "Iteration: 12000, Loss:  0.0554, enc_lr: [0.0001]\n",
      "Iteration: 13000, Loss:  0.0521, enc_lr: [0.0001]\n",
      "Iteration: 14000, Loss:  0.0379, enc_lr: [0.0001]\n",
      "Iteration: 15000, Loss:  0.0288, enc_lr: [0.0001]\n",
      "Iteration: 16000, Loss:  0.0233, enc_lr: [0.0001]\n",
      "Iteration: 17000, Loss:  0.0195, enc_lr: [0.0001]\n",
      "Iteration: 18000, Loss:  0.0192, enc_lr: [0.0001]\n",
      "Iteration: 19000, Loss:  0.0231, enc_lr: [0.0001]\n",
      "Iteration: 20000, Loss:  0.0151, enc_lr: [0.0001]\n",
      "Iteration: 21000, Loss:  0.0160, enc_lr: [1.0000000000000002e-06]\n",
      "Iteration: 22000, Loss:  0.0172, enc_lr: [1e-05]\n",
      "Iteration: 23000, Loss:  0.0184, enc_lr: [1e-05]\n",
      "Iteration: 24000, Loss:  0.0208, enc_lr: [1e-05]\n",
      "Iteration: 25000, Loss:  0.0140, enc_lr: [1e-05]\n",
      "Iteration: 26000, Loss:  0.0171, enc_lr: [1e-05]\n",
      "Iteration: 27000, Loss:  0.0194, enc_lr: [1e-05]\n",
      "Iteration: 28000, Loss:  0.0130, enc_lr: [1e-05]\n",
      "Iteration: 29000, Loss:  0.0274, enc_lr: [1e-05]\n",
      "Iteration: 30000, Loss:  0.0131, enc_lr: [1e-05]\n"
     ]
    }
   ],
   "source": [
    "#학습실행 \n",
    "trainIters(encoder, decoder, 30000, print_every=1000)  #매번 10000 => 0.001 ==> 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65b8cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), './models/encoder_seq2seq_attetion_dot_final.pth')\n",
    "torch.save(decoder.state_dict(), './models/decoder_seq2seq_attetion_dot_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15c617a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderLSTM(\n",
       "  (embedding): Embedding(433, 256)\n",
       "  (lstm): LSTM(256, 256, num_layers=2)\n",
       "  (out): Linear(in_features=256, out_features=433, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가실행\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6ff9928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'bye' to exit)\n",
      "> hello.\n",
      "< i really like it so today .\n",
      "> how are you?\n",
      "< i ' s promotion .\n",
      "> it is great!\n",
      "< it seems that it may rain today .\n",
      "> okay. thank you for letting me know\n",
      "< i took that .\n",
      "> What do you want to do?\n",
      "< i enjoy i go to go see you .\n",
      "> Come to see me\n",
      "< the first sounds good . i do it .\n",
      "> bye\n"
     ]
    }
   ],
   "source": [
    "chat(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce6f8821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 변수 로드\n",
    "with open('./dataset/vocab_seq2seq_attention_dot.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = vocab\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "encoder = EncoderLSTM(vocab_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderLSTM(hidden_size, vocab_size).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load('./models/chkpt/encoder_seq2seq_attention_dot_28000.pth'))\n",
    "decoder.load_state_dict(torch.load('./models/chkpt/decoder_seq2seq_attention_dot_28000.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d466014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'bye' to exit)\n",
      "> hi!\n",
      "< i really like it so today .\n",
      "> okay.\n",
      "< i really like it so today .\n",
      "> what?\n",
      "< i really like it so today .\n",
      "> what do you think today?\n",
      "< i ' s going to be .\n",
      "> how's today?\n",
      "< i really like it so today .\n",
      "> what do you like to do?\n",
      "< i ' ve always to draw and paint .\n",
      "> it's good.\n",
      "< i really like it so today .\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "chat(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43e8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
