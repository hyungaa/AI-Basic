{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a988e99e-2bd0-489c-8208-975ccc3a7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ffa1fa-f40b-4f2b-9d6c-a973d62d4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f17b06-3d6b-46dd-a8a9-58d5ae062c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^가-힣]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67870ae5-5fc1-4785-ad1d-c345a639d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U,W,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fe7361-bea7-40cb-bd1c-b1693bbab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1  # 각각의 word에 대한 one hot coding \n",
    "        hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i - 1]))\n",
    "        ys[i] = np.dot(V, hs[i])\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i]))  # softmax계산\n",
    "        loss += -np.log(ps[i][targets[i], 0])\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3930ee7e-bc5a-49ab-b578-9e9303fcf94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "\n",
    "    # Backward propagation through time (BPTT)\n",
    "    # 처음에 모든 가중치들은 0으로 설정\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # 매번 i스텝에서 dL/dVi를 구하기\n",
    "        dV_step_i = ps[i] @ (hs[i]).T  # (y_hat - y) @ hs.T - for each step\n",
    "\n",
    "        dV = dV + dV_step_i  # dL/dVi를 다 더하기\n",
    "\n",
    "        # 각i별로 V와 W를 구하기 위해서는\n",
    "        # 먼저 공통적으로 계산되는 부분을 delta로 해서 계산해두고\n",
    "        # 그리고 시간을 거슬러 dL/dWij와 dL/dUij를 구한 뒤\n",
    "        # 각각을 합하여 dL/dW와 dL/dU를 구하고 \n",
    "        # 다시 공통적으로 계산되는 delta를 업데이트\n",
    "\n",
    "        # i번째 스텝에서 공통적으로 사용될 delta\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
    "\n",
    "        # 시간을 거슬러 올라가서 dL/dW와 dL/dU를 구하\n",
    "        for j in range(i + 1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j - 1].T\n",
    "\n",
    "            dW = dW + dW_ij\n",
    "\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "\n",
    "            # 그리고 다음번 j번째 타임에서 공통적으로 계산할 delta를 업데이트\n",
    "            delta_recent = (W.T @ delta_recent) * (1 - hs[j - 1] ** 2)\n",
    "\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488c0f17-f587-4111-8910-430b6358259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_ix[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # 소프트맥스\n",
    "        ix = np.argmax(p)                    # 가장 높은 확률의 index를 리턴\n",
    "        x = np.zeros((vocab_size, 1))        # 다음번 input x를 준비\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix) \n",
    "    pred_words = ' '.join(ix_to_word[i] for i in ixes)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8275ef0d-d4be-4c52-ae70-0148eded7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 parameters\n",
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592c4adf-bb3b-41e7-99cd-3fb7e14ee447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231d7074-2b8e-4bb3-8fef-41cc3dcbae57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '못할',\n",
       " 1: '문자와',\n",
       " 2: '이런',\n",
       " 3: '많으니라',\n",
       " 4: '하고자',\n",
       " 5: '따름이니라',\n",
       " 6: '서로',\n",
       " 7: '할',\n",
       " 8: '이를',\n",
       " 9: '제',\n",
       " 10: '달라',\n",
       " 11: '펴지',\n",
       " 12: '날로',\n",
       " 13: '나라의',\n",
       " 14: '글자를',\n",
       " 15: '뜻을',\n",
       " 16: '하여',\n",
       " 17: '아니하기에',\n",
       " 18: '편안케',\n",
       " 19: '백성이',\n",
       " 20: '가엾이',\n",
       " 21: '까닭으로',\n",
       " 22: '사람이',\n",
       " 23: '이르고자',\n",
       " 24: '만드노니',\n",
       " 25: '여겨',\n",
       " 26: '어리석은',\n",
       " 27: '있어도',\n",
       " 28: '위해',\n",
       " 29: '내가',\n",
       " 30: '바가',\n",
       " 31: '스물여덟',\n",
       " 32: '마침내',\n",
       " 33: '사람마다',\n",
       " 34: '능히',\n",
       " 35: '쉬이',\n",
       " 36: '새로',\n",
       " 37: '말이',\n",
       " 38: '익혀',\n",
       " 39: '통하지',\n",
       " 40: '씀에',\n",
       " 41: '중국과'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda76d7d-7c5e-4c6a-b46e-c0e8c278884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8acdae2-f872-4e17-9188-855475e4cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.212484126784139\n",
      "epoch 100, loss: 2.035151302146545\n",
      "epoch 200, loss: 0.27357883135052274\n",
      "epoch 300, loss: 0.1185148078656201\n",
      "epoch 400, loss: 0.07069306509403017\n",
      "epoch 500, loss: 0.04966235292735596\n",
      "epoch 600, loss: 0.03792809698738463\n",
      "epoch 700, loss: 0.03046529176370589\n",
      "epoch 800, loss: 0.025357216048290235\n",
      "epoch 900, loss: 0.02165969780130683\n",
      "epoch 1000, loss: 0.01887046932236626\n",
      "epoch 1100, loss: 0.016702892406950148\n",
      "epoch 1200, loss: 0.014979233104231892\n",
      "epoch 1300, loss: 0.013581427868188263\n",
      "epoch 1400, loss: 0.012427257041077933\n",
      "epoch 1500, loss: 0.01145774021833132\n",
      "epoch 1600, loss: 0.010629734530459945\n",
      "epoch 1700, loss: 0.00991139692146385\n",
      "epoch 1800, loss: 0.009279385623501557\n",
      "epoch 1900, loss: 0.008716899290450369\n",
      "epoch 2000, loss: 0.008211991165324832\n",
      "epoch 2100, loss: 0.007756068988772536\n",
      "epoch 2200, loss: 0.007342706697375148\n",
      "epoch 2300, loss: 0.006966833034904858\n",
      "epoch 2400, loss: 0.006624245355549209\n",
      "epoch 2500, loss: 0.0063113459445134894\n",
      "epoch 2600, loss: 0.006025006562663301\n",
      "epoch 2700, loss: 0.005762495905596894\n",
      "epoch 2800, loss: 0.005521431458354306\n",
      "epoch 2900, loss: 0.005299736051366816\n",
      "epoch 3000, loss: 0.005095591746122067\n",
      "epoch 3100, loss: 0.004907391691140246\n",
      "epoch 3200, loss: 0.00473369497855108\n",
      "epoch 3300, loss: 0.004573190117935028\n",
      "epoch 3400, loss: 0.0044246701464867935\n",
      "epoch 3500, loss: 0.004287018659070255\n",
      "epoch 3600, loss: 0.004159203467440627\n",
      "epoch 3700, loss: 0.004040274162965011\n",
      "epoch 3800, loss: 0.003929360941361759\n",
      "epoch 3900, loss: 0.0038256734324629152\n",
      "epoch 4000, loss: 0.003728499170801719\n",
      "epoch 4100, loss: 0.003637201641660927\n",
      "epoch 4200, loss: 0.0035512178157715655\n",
      "epoch 4300, loss: 0.0034700550226344313\n",
      "epoch 4400, loss: 0.003393287031716533\n",
      "epoch 4500, loss: 0.0033205493068311867\n",
      "epoch 4600, loss: 0.0032515335191567354\n",
      "epoch 4700, loss: 0.003185981505780808\n",
      "epoch 4800, loss: 0.003123678924753716\n",
      "epoch 4900, loss: 0.0030644488825894004\n",
      "epoch 5000, loss: 0.0030081457997895595\n",
      "epoch 5100, loss: 0.002954649740112951\n",
      "epoch 5200, loss: 0.002903861367181266\n",
      "epoch 5300, loss: 0.002855697616768491\n",
      "epoch 5400, loss: 0.0028100880949238154\n",
      "epoch 5500, loss: 0.002766972139702359\n",
      "epoch 5600, loss: 0.002726296424313475\n",
      "epoch 5700, loss: 0.00268801293581816\n",
      "epoch 5800, loss: 0.0026520771388149258\n",
      "epoch 5900, loss: 0.002618446130349568\n",
      "epoch 6000, loss: 0.002587076613845428\n",
      "epoch 6100, loss: 0.00255792256971091\n",
      "epoch 6200, loss: 0.0025309325803675427\n",
      "epoch 6300, loss: 0.0025060468749710735\n",
      "epoch 6400, loss: 0.002483194282918216\n",
      "epoch 6500, loss: 0.0024622894033518587\n",
      "epoch 6600, loss: 0.002443230379061664\n",
      "epoch 6700, loss: 0.002425897674731124\n",
      "epoch 6800, loss: 0.0024101541806183777\n",
      "epoch 6900, loss: 0.002395846798508913\n",
      "epoch 7000, loss: 0.00238280945275887\n",
      "epoch 7100, loss: 0.0023708672615382455\n",
      "epoch 7200, loss: 0.00235984145559399\n",
      "epoch 7300, loss: 0.0023495545725189595\n",
      "epoch 7400, loss: 0.0023398354790312905\n",
      "epoch 7500, loss: 0.0023305238538783397\n",
      "epoch 7600, loss: 0.002321473866382824\n",
      "epoch 7700, loss: 0.002312556886329013\n",
      "epoch 7800, loss: 0.0023036631491283864\n",
      "epoch 7900, loss: 0.0022947023748079273\n",
      "epoch 8000, loss: 0.002285603402309661\n",
      "epoch 8100, loss: 0.002276312952978736\n",
      "epoch 8200, loss: 0.0022667936778806826\n",
      "epoch 8300, loss: 0.0022570216704830065\n",
      "epoch 8400, loss: 0.002246983637859096\n",
      "epoch 8500, loss: 0.0022366739208497287\n",
      "epoch 8600, loss: 0.0022260915399672392\n",
      "epoch 8700, loss: 0.0022152374239910407\n",
      "epoch 8800, loss: 0.0022041119566035055\n",
      "epoch 8900, loss: 0.002192712955266998\n",
      "epoch 9000, loss: 0.002181034175524608\n",
      "epoch 9100, loss: 0.0021690644109912047\n",
      "epoch 9200, loss: 0.0021567872320069226\n",
      "epoch 9300, loss: 0.0021441813730344998\n",
      "epoch 9400, loss: 0.0021312217406438743\n",
      "epoch 9500, loss: 0.002117880971897741\n",
      "epoch 9600, loss: 0.002104131429337996\n",
      "epoch 9700, loss: 0.0020899474766442687\n",
      "epoch 9800, loss: 0.002075307843165771\n",
      "epoch 9900, loss: 0.0020601978630950066\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for p in range(len(tokens)-seq_len):\n",
    "        inputs = [word_to_ix[tok] for tok in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_ix[tok] for tok in tokens[p + 1:p + seq_len + 1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086039d-cfd8-4573-915a-a8d07dcb1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "input:  나라의\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "달라 문자와 달라 문자와 서로 문자와 서로 통하지 서로 통하지 아니하기에 통하지 아니하기에 이런 아니하기에 이런 까닭으로 이런 까닭으로 어리석은 까닭으로 어리석은 백성이 어리석은 백성이 이르고자 백성이 이르고자 할 이르고자 할 바가 있어도 바가 있어도 마침내 있어도 마침내 제 마침내\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    try:\n",
    "        user_input = input(\"input: \")\n",
    "        if user_input == 'break':\n",
    "            break\n",
    "        response = predict(user_input,40)\n",
    "        print(response)\n",
    "    except:\n",
    "        print('Uh oh try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3162cb-4ba1-416d-9f1c-003121bd0cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
